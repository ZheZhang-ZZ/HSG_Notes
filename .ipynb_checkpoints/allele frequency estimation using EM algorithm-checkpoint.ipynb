{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data simulation\n",
    "\n",
    "#### Description of the scenario\n",
    "In this section, we would first simulate a scenario of sequencing which is composed of a population with 100 individuals who are genotyped at a single locus with different genotypes (0, 1, 2) across the population. The coverage (depth) of the sequencing ranges from 1 to 10 and the error of the sequencing is 0.05\n",
    "\n",
    "#### Step1: Real genotypes and allele frequcies\n",
    "In the first step, we can simulate the genotypes of all individuals and calculate the actual allele frequency $f$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.51"
      ],
      "text/latex": [
       "0.51"
      ],
      "text/markdown": [
       "0.51"
      ],
      "text/plain": [
       "[1] 0.51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(123) # set random seed to make the example can be reproduced\n",
    "size <- 100\n",
    "geno.real <- sample(0:2,size,replace=TRUE)\n",
    "f.real <- sum(geno.real)/(2*size) \n",
    "# f here is the allele frequency of \"2\" allele, which we defined as derived allele\n",
    "f.real # 0.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Sequencing simulation\n",
    "In this step, we will simulate a actual scenario of sequncing which just like sampling one ball each time from a jar with two balls (one is called \"0\", the other one is \"1\", just like two alleles), and the times of sampling is corresponding to the coverage of sequncing. Keep in mind that there is a sampling error (0.05) we must consider here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 10 × 100 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>⋯</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>⋯</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>⋯</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>⋯</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>⋯</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>⋯</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>⋯</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>⋯</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>⋯</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>⋯</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 10 × 100 of type dbl\n",
       "\\begin{tabular}{lllllllllllllllllllll}\n",
       "\t 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & ⋯ & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1\\\\\n",
       "\t 1 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 0 & ⋯ & 1 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1\\\\\n",
       "\t 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & ⋯ & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 1\\\\\n",
       "\t 1 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 1 & 0 & ⋯ & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1\\\\\n",
       "\t 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & ⋯ & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1\\\\\n",
       "\t 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & ⋯ & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 1\\\\\n",
       "\t 0 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 0 & ⋯ & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1\\\\\n",
       "\t 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & ⋯ & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1\\\\\n",
       "\t 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 0 & ⋯ & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1\\\\\n",
       "\t 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & ⋯ & 0 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 1 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 10 × 100 of type dbl\n",
       "\n",
       "| 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | ⋯ | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 |\n",
       "| 1 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | 0 | ⋯ | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 |\n",
       "| 1 | 1 | 1 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | ⋯ | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 1 | 1 |\n",
       "| 1 | 1 | 1 | 1 | 1 | 0 | 1 | 0 | 1 | 0 | ⋯ | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 |\n",
       "| 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | ⋯ | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 |\n",
       "| 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | ⋯ | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 1 |\n",
       "| 0 | 1 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | 0 | ⋯ | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 1 |\n",
       "| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 1 | 0 | ⋯ | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 |\n",
       "| 1 | 1 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | 0 | ⋯ | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 |\n",
       "| 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | ⋯ | 0 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 1 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n",
       " [1,] 0    1    1    1    1    1    1    1    1    0     ⋯     0     1    \n",
       " [2,] 1    0    1    0    1    1    1    1    1    0     ⋯     1     1    \n",
       " [3,] 1    1    1    0    1    1    1    1    0    0     ⋯     0     1    \n",
       " [4,] 1    1    1    1    1    0    1    0    1    0     ⋯     1     1    \n",
       " [5,] 1    1    1    1    1    0    0    0    1    0     ⋯     1     1    \n",
       " [6,] 1    1    1    1    1    0    0    0    1    0     ⋯     0     1    \n",
       " [7,] 0    1    1    0    1    1    1    1    1    0     ⋯     1     1    \n",
       " [8,] 1    1    1    1    1    1    1    0    1    0     ⋯     0     1    \n",
       " [9,] 1    1    1    0    1    1    1    1    1    0     ⋯     0     1    \n",
       "[10,] 1    1    1    1    1    0    0    0    1    0     ⋯     0     1    \n",
       "      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n",
       " [1,] 0     0     0     1     1     0     1     1    \n",
       " [2,] 0     0     0     1     1     0     1     1    \n",
       " [3,] 0     1     1     1     0     0     1     1    \n",
       " [4,] 0     0     0     1     0     0     1     1    \n",
       " [5,] 0     0     0     1     0     0     1     1    \n",
       " [6,] 0     1     0     1     0     1     1     1    \n",
       " [7,] 0     1     1     1     0     0     0     1    \n",
       " [8,] 0     0     0     1     1     0     1     1    \n",
       " [9,] 0     0     0     1     1     0     1     1    \n",
       "[10,] 1     1     1     1     0     0     1     1    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the function of sequencing\n",
    "sequencing <- function(geno, d, error){\n",
    "    # convert real genotypes to jars with 'balls'\n",
    "    if(geno==2){jar<-c(1,1)}\n",
    "    if(geno==1){jar<-c(0,1)}\n",
    "    if(geno==0){jar<-c(0,0)}\n",
    "    \n",
    "    # sample balls from jar based on coverage d \n",
    "    read <- sample(jar,d,replace=TRUE)\n",
    "    \n",
    "    # generate errors artificially based on the error rate\n",
    "    # here we assume that the error rate is the same across all the reads\n",
    "    read_with_error<-sapply(read,function(x)sample(c(x,1-x),1,prob=c(1-error,error)))\n",
    "    \n",
    "    return(read_with_error) \n",
    "}\n",
    "\n",
    "# Here we start with setting coverage to be 10 (d=10)                         \n",
    "reads<-sapply(geno.real,function(x)sequencing(x,d=10,error=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability model of allele frequency estimation\n",
    "\n",
    "#### Conditional independence assumptions\n",
    "\n",
    "Example 1.1 is a good start point to understand the idea of Bayesian statistics and basic assumptions of conditional independence. Here I will try to derive formula (1.1), which is very important for the following EM algorithm section.\n",
    "\n",
    "Consider that we have sequenced $n$ individuals at a specific locus, and the sequece data can be represented as ${\\boldsymbol \\rm d}=\\{d_1,\\dots,d_n\\}$. Here we have a unknown parameter vector ($\\boldsymbol \\theta$) comprised of allele frequency $f$, sequencing error $\\epsilon$, and we can construct a probability model based on joint probability of $\\{{\\boldsymbol \\rm d}, \\boldsymbol \\theta\\}$. Based on Bayes formula, we can get the following formula\n",
    "\n",
    "$$\n",
    "\\mathbb{P}({\\boldsymbol \\rm d}, \\boldsymbol \\theta)=\\mathbb{P}({\\boldsymbol \\rm d}|\\boldsymbol \\theta)\\mathbb{P}(\\boldsymbol \\theta) \\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol \\theta = \\{f, \\epsilon\\}$, substitute this into formula (1), we get\n",
    "\n",
    "$$\n",
    "\\mathbb{P}({\\boldsymbol \\rm d}, f, \\epsilon)=\\mathbb{P}({\\boldsymbol \\rm d}|f, \\epsilon)\\mathbb{P}(f, \\epsilon) \\tag{2}\n",
    "$$\n",
    "\n",
    "Formula (2) is just the first part of formula (1.1) in the book that as was shown below:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}({\\boldsymbol \\rm d}, f, \\epsilon)=\\mathbb{P}({\\boldsymbol \\rm d}|f, \\epsilon)\\mathbb{P}(f, \\epsilon)=\\left[\\prod_{i=1}^{n}\\sum_{g_{i}}\\mathbb{P}(d_i|g_i,\\epsilon)\\mathbb{P}(g_i|f)\\right]\\mathbb{P}(f, \\epsilon)\n",
    "$$\n",
    "\n",
    "It is a little bit confusing that how to convert $\\mathbb{P}({\\boldsymbol \\rm d}|f, \\epsilon)$ to $\\left[\\prod_{i=1}^{n}\\sum_{g_{i}}\\mathbb{P}(d_i|g_i,\\epsilon)\\mathbb{P}(g_i|f)\\right]$, and we will derive it as follows:\n",
    "\n",
    "Firstly, because the sequecing for different individuals are independent, so we can get the following formula based on the product role of probabilities:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}({\\boldsymbol \\rm d}|f, \\epsilon)=\\prod_{i=1}^{n}\\mathbb{P}(d_{i}|f, \\epsilon) \\tag{3}\n",
    "$$\n",
    "\n",
    "For each individual $i$, we can now put a new variable $g_i$, which is the unobserved genotype for this individual, in $\\mathbb{P}(d_{i}|f, \\epsilon)$, and then we can just get rid of this unobservable variable by integral, and this trick is just how we get the marginal distribution of a variable (eg. consider two variables $\\{{\\boldsymbol \\rm x},{\\boldsymbol \\rm y}\\}$ and we can get the marginal probability of ${\\boldsymbol \\rm x}$ by the integral computation: $\\mathbb{P}({\\boldsymbol \\rm x})=\\int \\mathbb{P}({\\boldsymbol \\rm x}, {\\boldsymbol \\rm y}){\\mathrm{d}{\\boldsymbol \\rm x}}$). As integral is for continuous variable and $g$ is actually a discrete varaible, so \"integral\" for $g$ is just summation. For one individual, the formula which can be presented below:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(d_{i}|f, \\epsilon)&=\\sum_{g_i}\\mathbb{P}(d_{i},g_i|f, \\epsilon) \\\\\n",
    "&=\\sum_{g_i}\\mathbb{P}(d_{i}|g_i,f, \\epsilon)\\mathbb{P}(g_i|f,\\epsilon)\n",
    "\\end{aligned} \\tag{4}\n",
    "$$\n",
    "\n",
    "Formula (4) is still not the same as the part after summation symbol in the square brackets in formula (1.1). In fact, there is a trick here called **conditonal independence** assumed here. When $g_i$ has been known $d_i$ and $f$ is independent, which means that $\\mathbb{P}(d_{i}|g_i,f, \\epsilon)=\\mathbb{P}(d_{i}|g_i,\\epsilon)$, and $\\mathbb{P}(g_i|f,\\epsilon)=\\mathbb{P}(g_i|f)$, which is due to that $g_i$ and $\\epsilon$ is independent based on the assumption of the scenario. Therefore, formula can be converted to the following formula:\n",
    "\n",
    "$$\n",
    "\\sum_{g_i}\\mathbb{P}(d_{i}|g_i,f, \\epsilon)\\mathbb{P}(g_i|f,\\epsilon) = \\sum_{g_i}\\mathbb{P}(d_{i}|g_i,\\epsilon)\\mathbb{P}(g_i|f) \\tag{5}\n",
    "$$\n",
    "\n",
    "Now, substitute (5) into (3), we get:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}({\\boldsymbol \\rm d}|f, \\epsilon)=\\prod_{i=1}^{n}\\sum_{g_i}\\mathbb{P}(d_{i}|g_i,\\epsilon)\\mathbb{P}(g_i|f) \\tag{6}\n",
    "$$\n",
    "\n",
    "Again, substitute (6) into (2), we get the same formula (1.1) in the book as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}({\\boldsymbol \\rm d}, f, \\epsilon)=\\mathbb{P}({\\boldsymbol \\rm d}|f, \\epsilon)\\mathbb{P}(f, \\epsilon)=\\left[\\prod_{i=1}^{n}\\sum_{g_{i}}\\mathbb{P}(d_i|g_i,\\epsilon)\\mathbb{P}(g_i|f)\\right]\\mathbb{P}(f, \\epsilon) \\tag{7}\n",
    "$$\n",
    "\n",
    "If we look into the equation (7) from Bayesian view, we can get that $\\mathbb{P}(f, \\epsilon)$ is the **prior probability** of $\\boldsymbol \\theta$, $\\left[\\prod_{i=1}^{n}\\sum_{g_{i}}\\mathbb{P}(d_i|g_i,\\epsilon)\\mathbb{P}(g_i|f)\\right]$ is called **likelihood** and $\\mathbb{P}({\\boldsymbol \\rm d}, f, \\epsilon)$ is the **posterior probability**. \n",
    "\n",
    "#### Probability Distributions\n",
    "\n",
    "For the likelihood in equation (7), we need to specified two probability funcitons, $\\mathbb{P}(d_i|g_i,\\epsilon)$ and $\\mathbb{P}(g_i|f)$. Firstly, if we known $f$，then we can estimate the probability of  genotype using H-W equilibrium:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(g_i|f)=\\tbinom{2}{g_i}f^{g_i}(1-f)^{g_i}=\\left\\{\n",
    "\\begin{aligned}\n",
    "&(1-f)^2  &\\text{ if } g_i=0 \\text{,}\\\\\n",
    "&2f(1-f)  &\\text{ if } g_i=1 \\text{,}\\\\\n",
    "&f^2      &\\text{ if } g_i=2 \\text{,}\n",
    "\\end{aligned}\n",
    "\\right. \\tag{8}\n",
    "$$\n",
    "where you can see that the piecewise function was converted to a single-line equation nicely. The function of $\\mathbb{P}(d_i|g_i,\\epsilon)$ can be represented below:\n",
    "$$\n",
    "\\mathbb{P}(d_i|g_i,\\epsilon)=\\prod_{j=1}^{n_i}\\mathbb{P}(b_{ij}|g_i,\\epsilon) \\tag{9}\n",
    "$$\n",
    "where  $b_{ij}$ is the *j*th read for *i*th individual. We can obtain this equation because we assume that reads for each individual were independent from each other, and $\\mathbb{P}(b_{ij}|g_i,\\epsilon)$ can be further converted to equation (10):\n",
    "$$\n",
    "\\mathbb{P}(b_{ij}|g_i,\\epsilon)=\\left\\{\n",
    "\\begin{aligned}\n",
    "&(1-\\epsilon)  &\\text{ if } g_i=0, b_{ij}=\\text{A} \\text{ or } g_i=2, b_{ij}=\\text{D} \\text{,}\\\\\n",
    "&\\frac{1}{2}  &\\text{ if } g_i=1 \\text{,}\\\\\n",
    "&\\epsilon      &\\text{ if } g_i=0, b_{ij}=\\text{D} \\text{ or } g_i=2, b_{ij}=\\text{A} \\text{,}\n",
    "\\end{aligned}\n",
    "\\right. \\tag{10}\n",
    "$$\n",
    "wehre **A** is the ancestral allele (which can just be denoted as \"REF\" allele in VCF file), and **D** is the derived allele (which can just be denoted as \"ALT\" allele in VCF file). For example, if we get a sequence result of first indiviual as $d_1=\\{\\text{A, D, A}\\}$, and if the acual genotype of individual is \"0\", this means the sequence result of **D** is an error, because we cannot get a derive allele from two ancestral alleles. Therefore, we can calculate the genotype likelihood for this individual as $\\mathbb{P}(d_1|g_1=0,\\epsilon)=(1-\\epsilon)*\\epsilon*(1-\\epsilon)=(1-\\epsilon)^2\\epsilon$\n",
    "\n",
    "There is actually a prior prbability $\\mathbb{P}(f, \\epsilon)$ in equation 7. However, if we don't want to estimate parameters using Bayesian inference, we can ignore it at present.\n",
    "\n",
    "### An introduction to EM algorithm\n",
    "\n",
    "In this section, we will firstly make a simple introduction to EM algorithm. Please note that we will not dive into the whole mathematical theory, but highlight some key concepts to help you understand this classic algorithm and to make a foundation for its application in estimation of allele frequencies.\n",
    "\n",
    "#### Definition\n",
    "\n",
    "EM (expectation-maximization) algorithm was proposed to handle the maximum likelihood estimation (MLE) of parametres in likelihood function with latent variables (which is genotype in our example). Below is the formula of EM:\n",
    "$$\n",
    "\\boldsymbol \\theta^{(t+1)}=\\mathop{\\arg\\max}_{\\boldsymbol \\theta}\\int_Z\\left[\\log(\\mathbb{P}(\\boldsymbol {X,Z|\\theta}))\\right]\\mathbb{P}(\\boldsymbol {Z|X,\\theta^{(t)}}) \\tag{11}\n",
    "$$\n",
    "Let's explain a little bit of equation 11. Firstly, EM algorithm is a iteration procedure, so $t$ means current iteration and $t+1$ means the next iteration. Our aim is to find MLE of parameters for the next round of iteration until the procedure converges. Secondly,  $\\mathbb{P}(\\boldsymbol {Z|X,\\theta^{(t)}})$ is called posterior probability of $\\boldsymbol {\\rm Z}$, where $\\boldsymbol {\\rm Z}$ is the latent variable, $\\boldsymbol {\\rm X}$ is the observaed data, and $\\boldsymbol {\\rm \\theta}$ is the unknown parameters. $\\log(\\mathbb{P}(\\boldsymbol {X,Z|\\theta}))$ is called log likelihood of complete data, and $\\int_Z\\left[\\log(\\mathbb{P}(\\boldsymbol {X,Z|\\theta}))\\right]\\mathbb{P}(\\boldsymbol {Z|X,\\theta^{(t)}})$ is the expectation of $\\log(\\mathbb{P}(\\boldsymbol {X,Z|\\theta}))$ in the space of $\\boldsymbol {\\rm Z}$. Why is it a expectation?\n",
    "\n",
    "We all know that for a random variable of $\\boldsymbol {\\rm Z}$, the expectation of $\\boldsymbol {\\rm Z}$ is $\\mathbb{E}\\left[\\boldsymbol {\\rm Z}\\right]=\\sum{z\\mathbb{P}(z)}$, when $\\boldsymbol {\\rm Z}$ is a discrete variable and $\\mathbb{E}\\left[\\boldsymbol {\\rm Z}\\right]=\\int{z\\mathbb{P}(z)}\\mathrm{d}z$, when $\\boldsymbol {\\rm Z}$ is a continuous variable. The regulation here is that the integral of a random variable multiplies the pdf (pmf) is actually the expectation. Therefore, $\\int_Z\\left[\\log(\\mathbb{P}(\\boldsymbol {X,Z|\\theta}))\\right]\\mathbb{P}(\\boldsymbol {Z|X,\\theta^{(t)}})$ can be regarded as $\\mathbb{E}_{\\boldsymbol Z}\\left[\\log(\\mathbb{P}(\\boldsymbol {X,Z|\\theta}))\\right]$, which is referred as E-step in EM algorithm. Meanwhile, $\\mathop{\\arg\\max}_{\\boldsymbol \\theta}\\mathbb{E}_{\\boldsymbol Z}\\left[\\log(\\mathbb{P}(\\boldsymbol {X,Z|\\theta}))\\right]$ is called M-step because it is used to maximize the expectation.\n",
    "\n",
    "#### Proof of convergence\n",
    "\n",
    "In the book, the authors listed the simple proof of the convergence of EM algorithm, which is not clear. Here we will give the details of the proof:\n",
    "\n",
    "The convergence of EM means that the next round of log likelihood of the data given parameters are always larger than the current those of current round, i.e., $\\log{\\mathbb{P}(\\boldsymbol X|\\boldsymbol \\theta^{(t+1)})}>\\log{\\mathbb{P}(\\boldsymbol X|\\boldsymbol \\theta^{(t)})}$.\n",
    "\n",
    "Based on Bayes rule, we get:\n",
    "$$\n",
    "\\log{\\mathbb{P}(\\boldsymbol x|\\boldsymbol\\theta)}=\\log{\\mathbb{P}(\\boldsymbol {x,z}|\\boldsymbol\\theta)} - \\log{\\mathbb{P}(\\boldsymbol z|\\boldsymbol{x,\\theta)}} \\tag{12}\n",
    "$$\n",
    "Now let us calculate the Expectation for the left and righ hand side based on $\\boldsymbol {Z|X,\\theta^{(t)}}$, for the left hand side:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{z|x,\\theta^{(t)}}\\left[\\log\\mathbb{P}(\\boldsymbol {x|\\theta})\\right]&=\\int_{\\boldsymbol z}\\left[\\log(\\mathbb{P}(\\boldsymbol {x|\\theta}))\\right]\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z \\\\\n",
    "&=\\log(\\mathbb{P}(\\boldsymbol {x|\\theta}))\\int_{\\boldsymbol z}\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z \\\\\n",
    "&=\\log(\\mathbb{P}(\\boldsymbol {x|\\theta})) \\times 1 \\\\\n",
    "&= \\log(\\mathbb{P}(\\boldsymbol {x|\\theta}))\n",
    "\\end{aligned} \\tag{13}\n",
    "$$\n",
    "The first line of eq.13 is the definition of Expectation. The second line holds because $\\log(\\mathbb{P}(\\boldsymbol {x|\\theta}))$ is not related to $\\boldsymbol Z$ and can be taken outside the $\\int$ directly. The third line holds because $\\int_{\\boldsymbol z}\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z$ is just the whole area of a probability density function (pdf), which is 1 from the definition. \n",
    "\n",
    "For the right hand side, we get:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{z|x,\\theta^{(t)}}\\left[\\log\\mathbb{P}(\\boldsymbol {x,z|\\theta})\\right]-\\mathbb{E}_{z|x,\\theta^{(t)}}\\left[\\log\\mathbb{P}(\\boldsymbol {z|x,\\theta})\\right]&=\\int_{\\boldsymbol z}\\left[\\log(\\mathbb{P}(\\boldsymbol {x,z|\\theta}))\\right]\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z - \\\\&\\int_{\\boldsymbol z}\\left[\\log(\\mathbb{P}(\\boldsymbol {z|x,\\theta}))\\right]\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z\\\\\n",
    "\\end{aligned} \\tag{14}\n",
    "$$\n",
    "Let's define $Q(\\boldsymbol {\\theta,\\theta^{(t)}})=\\int_{\\boldsymbol z}\\left[\\log(\\mathbb{P}(\\boldsymbol {x,z|\\theta}))\\right]\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z$, and $H(\\boldsymbol {\\theta,\\theta^{(t)}})=\\int_{\\boldsymbol z}\\left[\\log(\\mathbb{P}(\\boldsymbol {z|x,\\theta}))\\right]\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z$\n",
    "\n",
    "Now, let us recall the aim of this proof, i.e., $\\log{\\mathbb{P}(\\boldsymbol X|\\boldsymbol \\theta^{(t+1)})}>\\log{\\mathbb{P}(\\boldsymbol X|\\boldsymbol \\theta^{(t)})}$. If we can prove that $Q(\\boldsymbol {\\theta^{(t+1)},\\theta^{(t)}})\\geq Q(\\boldsymbol {\\theta,\\theta^{(t)}}) \\text{ && } H(\\boldsymbol {\\theta^{(t+1)},\\theta^{(t)}})\\leq H(\\boldsymbol {\\theta,\\theta^{(t)}})$, then it means $\\log{\\mathbb{P}(\\boldsymbol X|\\boldsymbol \\theta^{(t+1)})}>\\log{\\mathbb{P}(\\boldsymbol X|\\boldsymbol \\theta^{(t)})}$ holds.\n",
    "\n",
    "Firstly, we can find that $Q$ function is acutually the expectation in eq. 11 (EM algorithm) that we want to maximize. Now that we are maximizing $Q$ function during iteration, so the next round of $Q$ is must be larger or equal than the current round, which means that $Q(\\boldsymbol {\\theta^{(t+1)},\\theta^{(t)}})\\geq Q(\\boldsymbol {\\theta,\\theta^{(t)}})$ holds. \n",
    "\n",
    "Secondly, we need to prove $H(\\boldsymbol {\\theta^{(t+1)},\\theta^{(t)}})\\leq H(\\boldsymbol {\\theta,\\theta^{(t)}})$, i.e., $H(\\boldsymbol {\\theta^{(t+1)},\\theta^{(t)}}) - H(\\boldsymbol {\\theta^{(t)},\\theta^{(t)}}) \\leq0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&H(\\boldsymbol {\\theta^{(t+1)},\\theta^{(t)}}) - H(\\boldsymbol {\\theta^{(t)},\\theta^{(t)}}) \\\\\n",
    "&=\\int_{\\boldsymbol z}\\left[\\log(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t+1)}}))\\right]\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z-\\int_{\\boldsymbol z}\\left[\\log(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))\\right]\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z \\\\\n",
    "&=\\int_{\\boldsymbol z}\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\left[\\frac{\\log(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t+1)}}))}{\\log(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))}\\right]\\mathrm{d}\\boldsymbol z = \\mathbb{E}_{z|x,\\theta^{(t)}}\\left[\\frac{\\log(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t+1)}}))}{\\log(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))}\\right]\\\\\n",
    "&=\\mathbb{E}_{z|x,\\theta^{(t)}}\\log\\left[\\frac{(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t+1)}}))}{(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))}\\right]\n",
    "\\end{aligned} \\tag{15}\n",
    "$$\n",
    "Here we need a famous inequality, called *Jensen's inequality* to help complete the proof. Jensen inequality can be easily illustrated in a figure:\n",
    "\n",
    "![Jensen's equality](\"./Jensen_equality.png\" \"Jensen's inequality\")\n",
    "\n",
    "\n",
    "This figure means that for a concave function, e.g., log function, $\\mathbb{E}[f(x)]\\le f(\\mathbb{E}[x])$, which can be observed in the figure above that the line is always below the curve. With Jensen's inequality, we can obtain:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[f(x)]=\\mathbb{E}_{z|x,\\theta^{(t)}}\\log\\left[\\frac{(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t+1)}}))}{(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))}\\right] \\le f(\\mathbb{E}[x])&=\\log{\\mathbb{E}_{z|x,\\theta^{(t)}}}\\left[\\frac{(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t+1)}}))}{(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))}\\right]\\\\\n",
    "&=\\log\\int_{z}\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))\\frac{(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t+1)}}))}{(\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))}\\mathrm{d}z\\\\\n",
    "&=\\log\\int_{z}\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}}))\\mathrm{d}z\\\\\n",
    "&=\\log1=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "So, $H(\\boldsymbol {\\theta^{(t+1)},\\theta^{(t)}}) - H(\\boldsymbol {\\theta^{(t)},\\theta^{(t)}}) \\leq0$ holds, and hence $\\log{\\mathbb{P}(\\boldsymbol X|\\boldsymbol \\theta^{(t+1)})}>\\log{\\mathbb{P}(\\boldsymbol X|\\boldsymbol \\theta^{(t)})}$, i.e., EM algorithm can always converge.\n",
    "\n",
    "#### Steps of EM\n",
    "\n",
    "Eq.11 has actually illustrated the two steps included in the implementation of EM:\n",
    "\n",
    "+ E-step: Given the current $\\boldsymbol \\theta^{(t)}$, calculate the expecatation, i.e., $Q(\\boldsymbol {\\theta,\\theta^{(t)}})=\\int_{\\boldsymbol z}\\left[\\log(\\mathbb{P}(\\boldsymbol {x,z|\\theta}))\\right]\\mathbb{P}(\\boldsymbol {z|x,\\theta^{(t)}})\\mathrm{d}\\boldsymbol z$\n",
    "\n",
    "+ M-step: Optimize $\\boldsymbol \\theta$ by maximizing the $Q$ function, and then take $\\boldsymbol \\theta$ of this round to the next round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
